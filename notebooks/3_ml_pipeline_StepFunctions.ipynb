{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "matched-duration",
   "metadata": {},
   "source": [
    "## 機械学習パイプラインを構築した前処理、学習、推論の自動化\n",
    "\n",
    "このノートブックでは、AWS StepFunctions を活用し、2 で活用した前処理や、学習、推論といったそれぞれのジョブを一つのパイプラインにまとめます。その際、エラーハンドリングを行う機構も構築します。ワークフローの定義を Python で記述できる [AWS StepFunctions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/) を使います。ワークフローは構築したパイプラインはそのまま AWS 上に構築することができ、CloudFormation のテンプレートとしてもエクスポートできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-manhattan",
   "metadata": {},
   "source": [
    "## AWS StepFunctions Data Science SDK のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-sauce",
   "metadata": {},
   "source": [
    "## データの準備、s3 へアップロード\n",
    "今回も元のデータが S3 に保存されている状態からパイプラインを開始したいと思います。また、実行環境も前回と同様に前処理用のコンテナを準備氏、学習と推論用には AWS が提供している `scikit-learn` が事前にインストールされているコンテナを活用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-ocean",
   "metadata": {},
   "source": [
    "### 使用するライブラリなどの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import io\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep, TransformStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.template import TrainingPipeline\n",
    "from stepfunctions.template.utils import replace_parameters_with_jsonpath\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-meditation",
   "metadata": {},
   "source": [
    "AWS StepFunctions が各ジョブを実行する権限は、`TitanicPipelineAppStack-stepfunctionsworkflowexect` で定義されているので、AWS コンソールから確認して下記に ARN を記入下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "workflow_execution_role = \"arn:aws:iam::815969174475:role/TitanicPipelineAppStack-stepfunctionsworkflowexect-1B4SVGPRS0T9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-magic",
   "metadata": {},
   "source": [
    "### S3 へのデータのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "input_train = sagemaker_session.upload_data(path='./data/train.csv', key_prefix='kaggle-ml-pipeline/data')\n",
    "input_test = sagemaker_session.upload_data(path='./data/test.csv', key_prefix='kaggle-ml-pipeline/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-gasoline",
   "metadata": {},
   "source": [
    "### データ前処理用のコンテナの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t sagemaker-kaggle-titanic-preprocess ./scripts/preprocess\n",
    "\n",
    "import boto3\n",
    "\n",
    "# boto3の機能を使ってリポジトリ名に必要な情報を取得する\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "tag = ':latest'\n",
    "\n",
    "# SageMakerFullAccess を使っているから repository 名の中に sagemaker が含まれている必要がある\n",
    "ecr_repository = f'sagemaker-kaggle-titanic-preprocess'\n",
    "image_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repository+tag}'\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    " \n",
    "# リポジトリの作成\n",
    "# すでにある場合はこのコマンドは必要ない\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    " \n",
    "!docker build -t {ecr_repository} .\n",
    "!docker tag {ecr_repository + tag} $image_uri\n",
    "!docker push $image_uri\n",
    "\n",
    "print(f'コンテナは {image_uri} へ登録されています。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-norfolk",
   "metadata": {},
   "source": [
    "## ワークフロー定義の準備\n",
    "ワークフローへ渡す設定について、スキーマを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker へは学習ジョブ、モデル、エンドポイントへそれぞれユニークな名前を用います。 \n",
    "execution_input = ExecutionInput(\n",
    "    schema={\n",
    "    'ModelName': str,\n",
    "    'ModelDataUrl': str,\n",
    "    'TrainPreprocessingJobName': str,\n",
    "    'TrainingJobName': str, \n",
    "    'TestPreprocessingJobName': str,\n",
    "    'TransformJobName': str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-howard",
   "metadata": {},
   "source": [
    "### 学習ステップの定義\n",
    "前回行った、前処理と学習と同様の内容を実施しますが、今回はノートブック経由で SDK にてジョブを発行するのではなく、それぞれのステップとして定義し、最後にワークフローへと統合していきます。前処理ジョブの定義は前回と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'sagemaker-kaggle-preprocessing-train'\n",
    "output_s3_path = 's3://' + sagemaker_session.default_bucket() + '/kaggle-ml-pipeline'\n",
    "\n",
    "processing_input_dir = '/opt/ml/processing/input'\n",
    "processing_code_dir = '/opt/ml/processing/input/code'\n",
    "processing_output_dir = '/opt/ml/processing/output'\n",
    "\n",
    "\n",
    "PREPROCESSING_SCRIPT_LOCATION = './scripts/preprocess/preprocess_script/preprocess.py'\n",
    "\n",
    "input_code = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket=sagemaker_session.default_bucket(),\n",
    "    key_prefix= 'kaggle-ml-pipeline/preprocess/code',\n",
    ")\n",
    "\n",
    "output_s3_path_preprocess = output_s3_path + '/preprocessed'\n",
    "\n",
    "processor = ScriptProcessor(base_job_name=job_name,\n",
    "                                   image_uri=image_uri,\n",
    "                                   command=['python3'],\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.xlarge'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-uniform",
   "metadata": {},
   "source": [
    "定義した前処理ジョブをワークフローに組み込めるようにステップとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import ProcessingStep\n",
    "\n",
    "train_preprocess_step = ProcessingStep(\n",
    "    'Preprocess for Training Step', \n",
    "    processor=processor,\n",
    "    job_name=execution_input[\"TrainPreprocessingJobName\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_code, destination=processing_code_dir, input_name=\"code\"),\n",
    "        ProcessingInput(source=input_train, destination=processing_input_dir, input_name=\"train_data\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(source=processing_output_dir, destination=output_s3_path_preprocess, output_name=\"processed_train_data\")],\n",
    "    container_arguments=[\n",
    "                  '--data_type', 'train',\n",
    "                  '--input_dir',processing_input_dir,\n",
    "                  '--output_dir',processing_output_dir\n",
    "                      ],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/preprocess.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-commerce",
   "metadata": {},
   "source": [
    "学習ジョブの定義も前回と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "output_s3_path_train = output_s3_path + '/train'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='scripts/train/train.py',\n",
    "    framework_version=\"0.23-1\",\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_s3_path_train,\n",
    "    role=role)\n",
    "\n",
    "train_input = output_s3_path_preprocess + '/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-blocking",
   "metadata": {},
   "source": [
    "学習ジョブもステップとして定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    'Train Step', \n",
    "    estimator=sklearn,\n",
    "    data={'train': train_input},\n",
    "    job_name=execution_input['TrainingJobName']  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_step.get_expected_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-cuisine",
   "metadata": {},
   "source": [
    "### 推論ステップの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-festival",
   "metadata": {},
   "source": [
    "学習済モデルを推論で活用するために、モデル作成ステップを定義します。前回は Jupyter ノートブック上で学習ジョブと推論ジョブを呼び出していた(学習用ジョブの Estimator インスタンスをそのまま推論でも活用していた)ため、モデル作成を明示的には行っていませんでしたが、今回はパイプラインの中で独立したステップとして実行するために、モデル作成ステップを追加しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    state_id='Save model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName']  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-federation",
   "metadata": {},
   "source": [
    "推論用の前処理は前回と同様であり、こちらもステップとして定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'sagemaker-kaggle-preprocessing-test'\n",
    "\n",
    "processor = ScriptProcessor(base_job_name=job_name,\n",
    "                                   image_uri=image_uri,\n",
    "                                   command=['python3'],\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.xlarge'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess_step = steps.ProcessingStep(\n",
    "    'Preprocess for Test Step', \n",
    "    processor=processor,\n",
    "    job_name=execution_input[\"TestPreprocessingJobName\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_code, destination=processing_code_dir, input_name=\"code\"),\n",
    "        ProcessingInput(source=input_test, destination=processing_input_dir, input_name=\"test_data\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(source=processing_output_dir, destination=output_s3_path_preprocess, output_name=\"processed_test_data\")],\n",
    "    container_arguments=[\n",
    "                  '--data_type', 'test',\n",
    "                  '--input_dir',processing_input_dir,\n",
    "                  '--output_dir',processing_output_dir\n",
    "                      ],\n",
    "    container_entrypoint=[\"python3\", \"/opt/ml/processing/input/code/preprocess.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-savings",
   "metadata": {},
   "source": [
    "バッチ変換ジョブもステップとして定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_path_inference = output_s3_path + '/batch_inference'\n",
    "transformer = sklearn.transformer(instance_count=1,\n",
    "                                  instance_type='ml.m5.xlarge',\n",
    "                                  output_path=output_s3_path_inference)\n",
    "\n",
    "transform_step = steps.TransformStep(\n",
    "    'Transform Input Dataset',\n",
    "    transformer=sklearn.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.large'\n",
    "    ),\n",
    "    job_name=execution_input['TransformJobName'],     \n",
    "    model_name=execution_input['ModelName'], \n",
    "    data=output_s3_path_inference,\n",
    "    content_type='text/libsvm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-illustration",
   "metadata": {},
   "source": [
    "各ステップでのジョブが失敗した場合に対応する `Catch` ステップを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_processing_failure = stepfunctions.steps.states.Fail(\n",
    "    \"ML Workflow failed\", cause=\"SageMakerProcessingJobFailed\"\n",
    ")\n",
    "\n",
    "catch_state_processing = stepfunctions.steps.states.Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=failed_state_sagemaker_processing_failure,\n",
    ")\n",
    "\n",
    "train_preprocess_step.add_catch(catch_state_processing)\n",
    "training_step.add_catch(catch_state_processing)\n",
    "test_preprocess_step.add_catch(catch_state_processing)\n",
    "transform_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-citizen",
   "metadata": {},
   "source": [
    "各ステップをワークフローとして定義し、内容をレンダリングして確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([train_preprocess_step, training_step, model_step, test_preprocess_step, transform_step])\n",
    "#workflow_definition = steps.Chain([train_preprocess_step])\n",
    "\n",
    "\n",
    "workflow = Workflow(\n",
    "    name=\"titanic-ml-pipeline11\",\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    ")\n",
    "\n",
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-british",
   "metadata": {},
   "source": [
    "定義したワークフローは AWS CloudFormation のテンプレートとして出力できるため、Terraform など、サードパーティ OSS 含めた各種ツールでのデプロイにも流用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = workflow.get_cloudformation_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-charleston",
   "metadata": {},
   "source": [
    "### Workflow の実行\n",
    "`Chain` メソッドでまとめあげた各ステップを `workflow` として実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()\n",
    "\n",
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        \"ModelName\": 'model-{}'.format(uuid.uuid1().hex),\n",
    "        \"TrainPreprocessingJobName\": 'train-preprocess-job-{}'.format(uuid.uuid1().hex), \n",
    "        \"TrainingJobName\": 'training-job-{}'.format(uuid.uuid1().hex),\n",
    "        \"TestPreprocessingJobName\": 'test-preprocess-job-{}'.format(uuid.uuid1().hex),\n",
    "        \"TransformJobName\": 'transform-job-{}'.format(uuid.uuid1().hex),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-figure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-benjamin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
