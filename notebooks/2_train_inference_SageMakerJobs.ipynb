{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "administrative-invite",
   "metadata": {},
   "source": [
    "## AWS のサービスを使ったサーバレスな前処理、学習、推論\n",
    "\n",
    "このノートブックでは、AWS のマネージドな機械学習基盤サービスである、Amazon SageMaker を活用して、前処理や学習、推論を行います。SageMaker を活用すると、基本的にはデータは Amazon S3 に保存、実行環境はコンテナ化して Amazon ECR へ登録、処理の内容はほぼ従来どおり、Python などスクリプトで記述して、必要なタイミングでジョブを実行し、処理が終われば、インスタンスが停止してジョブが終わるというサーバーレスに近いジョブ実行環境を構築できます。それぞれのデータサイズが大きい、学習に時間がかかる、など Jupyter 環境では処理がしにくいような計算を行いたい場合には有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --quiet \"sagemaker>=2.19.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-termination",
   "metadata": {},
   "source": [
    "### 使用するライブラリなどの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "# SageMaker を活用するための権限が付与された Role を準備します。\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-albania",
   "metadata": {},
   "source": [
    "### Amazon S3 へのデータのアップロード\n",
    "\n",
    "データを S3 にアップロードし、AWS のサービスから活用する準備をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "input_train = sagemaker_session.upload_data(path='./data/train.csv', key_prefix='kaggle-ml-pipeline/data')\n",
    "input_test = sagemaker_session.upload_data(path='./data/test.csv', key_prefix='kaggle-ml-pipeline/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-restriction",
   "metadata": {},
   "source": [
    "## データの前処理\n",
    "### データ前処理用のジョブのためのコンテナの作成\n",
    "\n",
    "データの前処理や特徴量作成をジョブとして実行するために SageMaker Proccessing というサービス(詳細は後述)を使います。 SageMaker では組み込みコンテナとして [Apache Spark](https://docs.aws.amazon.com/sagemaker/latest/dg/use-spark-processing-container.html) と [scikit-learn](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html) などが準備されていますが、必要なライブラリを自分たちで持ち込んだコンテナを準備したくなる場合もあると思います。そういった場合には、独自のコンテナを活用することも可能なので、イメージを[作成](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-container-run-scripts.html)して活用します。手順は下記です。\n",
    "- 1. Jupyter ノートブック上でイメージをビルドする\n",
    "- 2. Amazon Elastic Container Registry の リポジトリにイメージを push する\n",
    "- 3. push したイメージを利用して前処理を行う\n",
    "\n",
    "活用するイメージは Dockerfile で定義し、`docker` コマンドでビルドします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./scripts/preprocess/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t sagemaker-kaggle-titanic-preprocess ./scripts/preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-timothy",
   "metadata": {},
   "source": [
    "コンテナをコンテナレジストリである、ECR へアップロードします。この際、AWS の python SDK である boto3 を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# boto3の機能を使ってリポジトリ名に必要な情報を取得する\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "tag = ':latest'\n",
    "\n",
    "# Repository 名の中に sagemaker が含まれている必要がある\n",
    "ecr_repository = f'sagemaker-kaggle-titanic-preprocess'\n",
    "image_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repository+tag}'\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    " \n",
    "# リポジトリの作成\n",
    "# すでにある場合はこのコマンドは必要ない\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    " \n",
    "!docker build -t {ecr_repository} .\n",
    "!docker tag {ecr_repository + tag} $image_uri\n",
    "!docker push $image_uri\n",
    "\n",
    "print(f'コンテナは {image_uri} へ登録されています。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-broadcast",
   "metadata": {},
   "source": [
    "これで前処理に必要な Docker イメージが準備できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-cloud",
   "metadata": {},
   "source": [
    "### 前処理を実行する SageMaker Processing ジョブを定義して実行\n",
    "Amazon SageMaker Processingは機械学習の前処理、後処理、モデル評価とそれぞれの計算処理をAmazon SageMaker の機能です。実行環境（コンテナ）を用意し、実行したい処理を記述し、実行するインスタンスタイプを指定することでジョブを実行することができます。コンテナと処理を記述したスクリプトを実行し、処理が終わるとインスタンスが停止するというシンプルな機能から、上述した機械学習関連のデータ処理にとどまらず、学習や推論にもお使い頂くような場合もあります。ジョブの定義や実行を行う Python SDK の詳細については[ドキュメント](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html)を確認して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'sagemaker-kaggle-preprocessing-train'\n",
    "processing_input_dir = '/opt/ml/processing/input'\n",
    "processing_output_dir = '/opt/ml/processing/output'\n",
    "output_s3_path = 's3://' + sagemaker_session.default_bucket() + '/kaggle-ml-pipeline'\n",
    "\n",
    "output_s3_path_preprocess = output_s3_path + '/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-environment",
   "metadata": {},
   "source": [
    "前処理を行うための Processor インスタンスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ScriptProcessor(\n",
    "                    base_job_name=job_name,\n",
    "                    image_uri=image_uri,\n",
    "                    command=['python3'],\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.c5.xlarge'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-details",
   "metadata": {},
   "source": [
    "Processing ジョブの実行をします。入力データに関しては、入力元の S3 のパスと、コンテナ上のどこに展開するかを定義、出力データには、コンテナ上のどのデータを S3 のどこに保存するかを定義。処理する内容は、[1_train_inference_notebook.ipynb](https://github.com/tkazusa/kaggle-mlpipeline-titanic/blob/main/notebooks/1_train_inference_notebook.ipynb) と同様のものを、スクリプト化し、`./scripts/preprocess/preprocess_script/` へ置いてあり、それを活用しています。スクリプトへ渡すコマンドライン引数は、 `run` メソッドの `arguments` 引数で定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    code='./scripts/preprocess/preprocess_script/preprocess.py', # S3 の URI でも可\n",
    "    inputs=[ProcessingInput(source=input_train, destination=processing_input_dir)],\n",
    "    outputs=[ProcessingOutput(source=processing_output_dir, destination=output_s3_path_preprocess)],\n",
    "    arguments=[\n",
    "          '--data_type', 'train',\n",
    "          '--input_dir',processing_input_dir,\n",
    "          '--output_dir',processing_output_dir\n",
    "              ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-private",
   "metadata": {},
   "source": [
    "この前処理ジョブによって `s3:<bucket-name>/kaggle-ml-pipeline/data/train.csv` にあった元のデータが、特徴量が追加されて、 `s3:<bucket-name>/kaggle-ml-pipeline/preprocessed/train.csv` へ保存されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-spiritual",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-works",
   "metadata": {},
   "source": [
    "機械学習モデルの学習には、SageMaker の学習ジョブを活用します。この学習ジョブは先程の前処理を行った Processing ジョブと異なり分散学習や実験管理など、学習に必要な機能を簡単に追加することもできます。そのため、学習用のジョブとして独立した機能が提供されています。今回は SageMaker が提供している、`scikit-learn` が事前にインストールされたコンテナを実行環境として使います。 Processing ジョブと同様に独自コンテナを持ち込むこともできます。詳細は[コチラ](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/docker-containers.html)をご確認下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "output_s3_path_train = output_s3_path + '/train'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='scripts/train/train.py',\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=output_s3_path_train,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = output_s3_path_preprocess + '/train.csv'\n",
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-infrared",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-borough",
   "metadata": {},
   "source": [
    "テスト用データにも前処理を施します。先程と同様のスクリプトとコンテナを活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'sagemaker-kaggle-preprocessing-test'\n",
    "\n",
    "processor = ScriptProcessor(base_job_name=job_name,\n",
    "                                   image_uri=image_uri,\n",
    "                                   command=['python3'],\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.xlarge'\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "processor.run(code='./scripts/preprocess/preprocess_script/preprocess.py', # S3 の URI でも可\n",
    "              inputs=[ProcessingInput(source=input_test, destination=processing_input_dir)],\n",
    "              outputs=[ProcessingOutput(source=processing_output_dir, destination=output_s3_path_preprocess)],\n",
    "              arguments=[\n",
    "                  '--data_type', 'test',\n",
    "                  '--input_dir',processing_input_dir,\n",
    "                  '--output_dir',processing_output_dir\n",
    "                      ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-crest",
   "metadata": {},
   "source": [
    "推論には SageMaker のバッチ変換ジョブを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_path_inference = output_s3_path + '/batch_inference'\n",
    "\n",
    "transformer = sklearn.transformer(\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.m5.xlarge',\n",
    "                        output_path=output_s3_path_inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = output_s3_path_preprocess + '/test.csv'\n",
    "\n",
    "transformer.transform(\n",
    "    data=test_input,\n",
    "    content_type='text/csv')\n",
    "\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-belfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-candy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-remains",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
